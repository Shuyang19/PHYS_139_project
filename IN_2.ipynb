{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dh_UlrUMuFn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import itertools\n",
        "\n",
        "def assign_matrices(N, Nr):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Create indices for all possible combinations of receivers and senders\n",
        "    indices = torch.tensor(list(itertools.product(range(N), range(N))), dtype=torch.long, device=device)\n",
        "\n",
        "    # Remove combinations where receiver and sender are the same\n",
        "    mask = indices[:, 0] != indices[:, 1]\n",
        "    indices = indices[mask]\n",
        "\n",
        "    # Initialize Rr and Rs tensors\n",
        "    Rr = torch.zeros(N, Nr, device=device)\n",
        "    Rs = torch.zeros(N, Nr, device=device)\n",
        "\n",
        "    # Assign 1 to appropriate positions in Rr and Rs\n",
        "    Rr[indices[:, 0], torch.arange(Nr)] = 1\n",
        "    Rs[indices[:, 1], torch.arange(Nr)] = 1\n",
        "\n",
        "    print(\"Rr:\", Rr)\n",
        "    print(\"Rs:\", Rs)\n",
        "    return Rr, Rs\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Rr, Rs = assign_matrices(5, 20)"
      ],
      "metadata": {
        "id": "RPTpAcmvuEZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_matrices_SV(N, Nt, Nv):\n",
        "    # Create indices for all possible combinations of keys and values\n",
        "    indices = torch.tensor(list(itertools.product(range(N), range(Nv))), dtype=torch.long)\n",
        "\n",
        "    # Initialize Rk and Rv tensors\n",
        "    Rk = torch.zeros(N, Nt)\n",
        "    Rv = torch.zeros(Nv, Nt)\n",
        "\n",
        "    # Assign 1 to appropriate positions in Rk and Rv\n",
        "    Rk[indices[:, 0], torch.arange(Nt)] = 1\n",
        "    Rv[indices[:, 1], torch.arange(Nt)] = 1\n",
        "\n",
        "    # Move tensors to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        Rk = Rk.cuda()\n",
        "        Rv = Rv.cuda()\n",
        "\n",
        "    return Rk,Rv\n",
        "\n",
        "Rk, Rv = assign_matrices_SV(3, 6, 2)\n",
        "\n",
        "print(\"Rk:\", Rk)\n",
        "# print(Rk @ Rv)\n",
        "print(\"Rv:\", Rv)\n",
        "\n"
      ],
      "metadata": {
        "id": "BjKoxBA7uIbj",
        "outputId": "dfddc7f9-9b07-487a-d2ef-469be705771c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rk: tensor([[1., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 1.]])\n",
            "Rv: tensor([[1., 0., 1., 0., 1., 0.],\n",
            "        [0., 1., 0., 1., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YynW7Og2tS1q"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import itertools\n",
        "import torch.nn as nn\n",
        "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, BatchNorm1d\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "\n",
        "class IN(torch.nn.Module):\n",
        "    def __init__(self, hidden=3, N_p=1000, params=60, N_v=1000, params_v=5, De=1, Do=6, softmax=False):\n",
        "        super(IN, self).__init__()  # Call the superclass's __init__ method\n",
        "        self.hidden = hidden\n",
        "        self.Np = N_p  # N\n",
        "        self.P = params\n",
        "        self.Npp = self.Np * (self.Np - 1)  # Nr\n",
        "        self.Nv = N_v\n",
        "        self.S = params_v\n",
        "        self.Npv = self.Np * self.Nv  # Nt\n",
        "        self.De = De\n",
        "        self.Do = Do\n",
        "        self.softmax = softmax\n",
        "\n",
        "        self.fr_pp = Seq(Lin(2 * self.P, self.hidden), ReLU(), Lin(self.hidden, self.De))\n",
        "        self.fr_pv = Seq(Lin(self.P + self.S, self.hidden), ReLU(), Lin(self.hidden, self.De))\n",
        "        self.fo = Seq(Lin(self.P + (2 * self.De), self.hidden), ReLU(), Lin(self.hidden, self.Do))\n",
        "        self.fc = nn.Linear(self.Do, 1)\n",
        "\n",
        "        self.Rr,self.Rs = assign_matrices(self.Np, self.Npp)\n",
        "        self.Rk,self.Rv = assign_matrices(self.Np, self.Npv, self.Nv)\n",
        "\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # PF Candidate\n",
        "        # Orr = self.tmul(x, Rr)\n",
        "        # Ors = self.tmul(x, Rs)\n",
        "        Orr = x @ self.Rr\n",
        "        Ors = x @ self.Rr\n",
        "        B = torch.cat([Orr, Ors], 1)\n",
        "\n",
        "        # First MLP\n",
        "        B = B.transpose(1, 2).contiguous()\n",
        "        E = self.fr_pp(B.view(-1, 2 * self.P))\n",
        "        del B\n",
        "\n",
        "        E = E.transpose(1, 2).contiguous()\n",
        "        # Ebar_pp = self.tmul(E, Rr.t().contiguous())\n",
        "        Ebar_pp = (E @ self.Rr.t().contiguous())\n",
        "        del E\n",
        "\n",
        "        # Secondary Vertex\n",
        "        # PF Candidate\n",
        "        #Ork = self.tmul(x, self.Rk)\n",
        "        #Orv = self.tmul(y, self.Rv)\n",
        "        Ork = x @ self.Rk\n",
        "        Orv = y @ self.Rv\n",
        "        B = torch.cat([Ork, Orv], 1)\n",
        "\n",
        "        # First MLP\n",
        "        B = B.transpose(1, 2).contiguous()\n",
        "        E = self.fr_pv(B.view(-1, self.S + self.P))\n",
        "        del B\n",
        "        E = E.transpose(1, 2).contiguous()\n",
        "\n",
        "        #Ebar_pv = self.tmul(E, torch.transpose(self.Rk, 0, 1).contiguous())\n",
        "        #Ebar_vp = self.tmul(E, torch.transpose(self.Rv, 0, 1).contiguous())\n",
        "        #Ebar_pv = (E @ self.Rk.t().contiguous())\n",
        "        #typo in text, see dimention of matrix\n",
        "        Ebar_vp = (E @ self.Rk.t().contiguous())\n",
        "        del E\n",
        "\n",
        "\n",
        "        # Final output maxtrix for particels\n",
        "        #C = torch.cat([x, Ebar_pp, Ebar_pv], 1)\n",
        "        #del Ebar_pp\n",
        "        #del Ebar_pv\n",
        "        #C = torch.transpose(C, 1, 2).contiguous()\n",
        "        C = torch.cat([x, Ebar_pp, Ebar_pv], 1).transpose(1, 2).contiguous()\n",
        "        del Ebar_pp\n",
        "        del Ebar_pv\n",
        "        # Second MLP\n",
        "        O = self.fo(C.view(self.P + (2 * self.De)))\n",
        "        del C\n",
        "\n",
        "\n",
        "        #Taking the sum of over each particle/vertex\n",
        "        N = torch.sum(O, dim=1)\n",
        "        del O\n",
        "\n",
        "\n",
        "        #Classification MLP\n",
        "        N = self.fc(N)\n",
        "\n",
        "        if self.softmax:\n",
        "            N = nn.Softmax(dim=-1)(N)\n",
        "\n",
        "\n",
        "        return N\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dsk8xpB_tpbx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}