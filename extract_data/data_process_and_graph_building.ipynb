{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import hdf5plugin\n",
    "import numpy as np\n",
    "import itertools\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "\n",
    "filename = \"/higgs/train/ntuple_merged_44.h5\"\n",
    "\n",
    "h5 = h5py.File(filename,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_2 = ['track_ptrel',     \n",
    "          'track_erel',     \n",
    "          'track_phirel',     \n",
    "          'track_etarel',     \n",
    "          'track_deltaR',\n",
    "          'track_drminsv',     \n",
    "          'track_drsubjet1',     \n",
    "          'track_drsubjet2',\n",
    "          'track_dz',     \n",
    "          'track_dzsig',     \n",
    "          'track_dxy',     \n",
    "          'track_dxysig',     \n",
    "          'track_normchi2',     \n",
    "          'track_quality',     \n",
    "          'track_dptdpt',     \n",
    "          'track_detadeta',     \n",
    "          'track_dphidphi',     \n",
    "          'track_dxydxy',     \n",
    "          'track_dzdz',     \n",
    "          'track_dxydz',     \n",
    "          'track_dphidxy',     \n",
    "          'track_dlambdadz',     \n",
    "          'trackBTag_EtaRel',     \n",
    "          'trackBTag_PtRatio',     \n",
    "          'trackBTag_PParRatio',     \n",
    "          'trackBTag_Sip2dVal',     \n",
    "          'trackBTag_Sip2dSig',     \n",
    "          'trackBTag_Sip3dVal',     \n",
    "          'trackBTag_Sip3dSig',     \n",
    "          'trackBTag_JetDistVal'\n",
    "         ]\n",
    "\n",
    "\n",
    "labels_qcd = {\"label_QCD_b\": 53, \"label_QCD_bb\":51, \"label_QCD_c\":54,\"label_QCD_cc\":52,\"label_QCD_others\": 55}\n",
    "label_H_bb = 41\n",
    "num_jets = h5[\"event_no\"].shape[0]\n",
    "n_particles = h5[\"track_dxydxy\"].shape[-1]\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "edge_indices = []\n",
    "for jet in range(num_jets):\n",
    "    if h5[\"fj_label\"][jet] in list(labels_qcd.values()):\n",
    "        y.append([0,1])\n",
    "    elif h5[\"fj_label\"][jet] == label_H_bb:\n",
    "        y.append([1,0])\n",
    "    else: continue\n",
    "    \n",
    "    x = []\n",
    "    for feature in params_2:\n",
    "        x.append(h5[feature][jet])\n",
    "    X.append(x)\n",
    "    \n",
    "    # complete graph has n_particles*(n_particles-1)/2 edges, here we double count each edge, so has  n_particles*(n_particles-1) total edges\n",
    "    pairs = np.stack([[m, n] for (m, n) in itertools.product(range(n_particles), range(n_particles)) if m != n])\n",
    "    # edge_index = torch.tensor(pairs, dtype=torch.long)\n",
    "    edge_index = pairs.transpose()\n",
    "    edge_indices.append(edge_index)\n",
    "    \n",
    "    if jet > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pytorch dataset\n",
    "datas = []\n",
    "i = 0\n",
    "data = Data(x=torch.tensor(X[i]), edge_index=torch.tensor(edge_indices[i]), y=torch.tensor(y[i], dtype=torch.long).unsqueeze(dim=0))\n",
    "for i in range(len(X)):\n",
    "    data = Data(x=torch.tensor(X[i]), edge_index=torch.tensor(edge_indices[i]), y=torch.tensor(y[i], dtype=torch.long).unsqueeze(dim=0))\n",
    "    datas.append([data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])],\n",
       " [Data(x=[30, 60], edge_index=[2, 3540], y=[1, 2])]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import EdgeConv, global_mean_pool\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU, BatchNorm1d\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = 60\n",
    "hidden = 128\n",
    "outputs = 2\n",
    "\n",
    "\n",
    "class EdgeBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EdgeBlock, self).__init__()\n",
    "        self.edge_mlp = Seq(Lin(inputs * 2, hidden), BatchNorm1d(hidden), ReLU(), Lin(hidden, hidden))\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        out = torch.cat([src, dest], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "\n",
    "class NodeBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NodeBlock, self).__init__()\n",
    "        self.node_mlp_1 = Seq(Lin(inputs + hidden, hidden), BatchNorm1d(hidden), ReLU(), Lin(hidden, hidden))\n",
    "        self.node_mlp_2 = Seq(Lin(inputs + hidden, hidden), BatchNorm1d(hidden), ReLU(), Lin(hidden, hidden))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        row, col = edge_index\n",
    "        out = torch.cat([x[row], edge_attr], dim=1)\n",
    "        out = self.node_mlp_1(out)\n",
    "        out = scatter_mean(out, col, dim=0, dim_size=x.size(0))\n",
    "        out = torch.cat([x, out], dim=1)\n",
    "        return self.node_mlp_2(out)\n",
    "\n",
    "\n",
    "class GlobalBlock(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalBlock, self).__init__()\n",
    "        self.global_mlp = Seq(Lin(hidden, hidden), BatchNorm1d(hidden), ReLU(), Lin(hidden, outputs))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        out = scatter_mean(x, batch, dim=0)\n",
    "        return self.global_mlp(out)\n",
    "\n",
    "\n",
    "class InteractionNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InteractionNetwork, self).__init__()\n",
    "        self.interactionnetwork = MetaLayer(EdgeBlock(), NodeBlock(), GlobalBlock())\n",
    "        self.bn = BatchNorm1d(inputs)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "\n",
    "        x = self.bn(x)\n",
    "        x, edge_attr, u = self.interactionnetwork(x, edge_index, None, None, batch)\n",
    "        return u\n",
    "\n",
    "\n",
    "model = InteractionNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, loader, total, batch_size, leave=False):\n",
    "    model.eval()\n",
    "\n",
    "    xentropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total / batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "        data = data.to(device)\n",
    "        y = torch.argmax(data.y, dim=1)\n",
    "        batch_output = model(data.x, data.edge_index, data.batch)\n",
    "        batch_loss_item = xentropy(batch_output, y).item()\n",
    "        sum_loss += batch_loss_item\n",
    "        t.set_description(\"loss = %.5f\" % (batch_loss_item))\n",
    "        t.refresh()  # to show immediately the update\n",
    "\n",
    "    return sum_loss / (i + 1)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loader, total, batch_size, leave=False):\n",
    "    model.train()\n",
    "\n",
    "    xentropy = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "\n",
    "    sum_loss = 0.0\n",
    "    t = tqdm(enumerate(loader), total=total / batch_size, leave=leave)\n",
    "    for i, data in t:\n",
    "        data = data.to(device)\n",
    "        y = torch.argmax(data.y, dim=1)\n",
    "        optimizer.zero_grad()\n",
    "        batch_output = model(data.x, data.edge_index, data.batch)\n",
    "        batch_loss = xentropy(batch_output, y)\n",
    "        batch_loss.backward()\n",
    "        batch_loss_item = batch_loss.item()\n",
    "        t.set_description(\"loss = %.5f\" % batch_loss_item)\n",
    "        t.refresh()  # to show immediately the update\n",
    "        sum_loss += batch_loss_item\n",
    "        optimizer.step()\n",
    "\n",
    "    return sum_loss / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, DataListLoader, Batch\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "def collate(items):\n",
    "    l = sum(items, [])\n",
    "    return Batch.from_data_list(l)\n",
    "\n",
    "\n",
    "train_dataset = datas\n",
    "valid_dataset = datas\n",
    "test_dataset = datas\n",
    "\n",
    "torch.manual_seed(0)\n",
    "valid_frac = 0.20\n",
    "# full_length = len(graph_dataset)\n",
    "# valid_num = int(valid_frac * full_length)\n",
    "batch_size = 32\n",
    "\n",
    "# train_dataset, valid_dataset = random_split(graph_dataset, [full_length - valid_num, valid_num])\n",
    "\n",
    "train_loader = DataListLoader(train_dataset, batch_size=batch_size, pin_memory=True, shuffle=True)\n",
    "train_loader.collate_fn = collate\n",
    "valid_loader = DataListLoader(valid_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)\n",
    "valid_loader.collate_fn = collate\n",
    "test_loader = DataListLoader(test_dataset, batch_size=batch_size, pin_memory=True, shuffle=False)\n",
    "test_loader.collate_fn = collate\n",
    "\n",
    "\n",
    "train_samples = len(train_dataset)\n",
    "valid_samples = len(valid_dataset)\n",
    "test_samples = len(test_dataset)\n",
    "# print(full_length)\n",
    "print(train_samples)\n",
    "print(valid_samples)\n",
    "print(test_samples)\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c48dd581f974569bea63027134f23d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7a37c170a7460cbe024cc1a85eed9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/0.375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/173199925.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {:02d}, Training Loss:   {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_129/737766029.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, loader, total, batch_size, leave)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import os.path as osp\n",
    "\n",
    "n_epochs = 10\n",
    "stale_epochs = 0\n",
    "best_valid_loss = 99999\n",
    "patience = 5\n",
    "t = tqdm(range(0, n_epochs))\n",
    "\n",
    "for epoch in t:\n",
    "    loss = train(model, optimizer, train_loader, train_samples, batch_size, leave=bool(epoch == n_epochs - 1))\n",
    "    valid_loss = test(model, valid_loader, valid_samples, batch_size, leave=bool(epoch == n_epochs - 1))\n",
    "    print(\"Epoch: {:02d}, Training Loss:   {:.4f}\".format(epoch, loss))\n",
    "    print(\"Validation Loss: {:.4f}\".format(valid_loss))\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        modpath = osp.join(\"interactionnetwork_best.pth\")\n",
    "        print(\"New best model saved to:\", modpath)\n",
    "        torch.save(model.state_dict(), modpath)\n",
    "        stale_epochs = 0\n",
    "    else:\n",
    "        print(\"Stale epoch\")\n",
    "        stale_epochs += 1\n",
    "    if stale_epochs >= patience:\n",
    "        print(\"Early stopping after %i stale epochs\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
